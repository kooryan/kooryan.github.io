## Research


<small>Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.

---

### [Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation](https://aclanthology.org/2024.emnlp-main.386/)
Karin De Langis, **Ryan Koo**, Dongyeop Kang  
*Empirical Methods in Natural Language Processing* (EMNLP) Main, 2024

### [Benchmarking Cognitive Biases in Large Language Models as Evaluators](https://arxiv.org/abs/2309.17012)
**Ryan Koo**, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang  
*Association for Computational Linguistics* (ACL) Findings, 2024


### [Meta-Crafting: Improved Detection of Out-of-Distributed Texts via Crafting Metadata Space](https://ojs.aaai.org/index.php/AAAI/article/view/30467)
**Ryan Koo**, Yekyung Kim, Dongyeop Kang, Jaehyung Kim  
*Association for the Advancement of Artificial Intelligence* (AAAI) Student Abstract, 2024

### [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://arxiv.org/abs/2305.09857)
Vipul Raheja, Dhruv Kumar, **Ryan Koo**, Dongyeop Kang  
*Empirical Methods in Natural Language Processing* (EMNLP) Findings, 2023

</small>