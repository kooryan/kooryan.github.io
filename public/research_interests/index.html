<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Ryan Koo</title>
<meta name="keywords" content="">
<meta name="description" content="Research
Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.

Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation
Karin De Langis, Ryan Koo, Dongyeop Kang
Empirical Methods in Natural Language Processing (EMNLP) Main, 2024">
<meta name="author" content="Ryan Koo">
<link rel="canonical" href="http://localhost:1313/research_interests/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e690afcd5c523330d5c8b4d746eb158361600a015e99518d4d246a6ccab0cc19.css" integrity="sha256-5pCvzVxSMzDVyLTXRusVg2FgCgFemVGNTSRqbMqwzBk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/research_interests/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="Research
Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.

Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation
Karin De Langis, Ryan Koo, Dongyeop Kang
Empirical Methods in Natural Language Processing (EMNLP) Main, 2024" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/research_interests/" /><meta property="article:section" content="" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Research
Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.

Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation
Karin De Langis, Ryan Koo, Dongyeop Kang
Empirical Methods in Natural Language Processing (EMNLP) Main, 2024"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "",
      "item": "http://localhost:1313/research_interests/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "Research Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.\nDynamic Multi-Reward Weighting for Multi-Style Controllable Generation Karin De Langis, Ryan Koo, Dongyeop Kang\nEmpirical Methods in Natural Language Processing (EMNLP) Main, 2024\n",
  "keywords": [
    
  ],
  "articleBody": "Research Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.\nDynamic Multi-Reward Weighting for Multi-Style Controllable Generation Karin De Langis, Ryan Koo, Dongyeop Kang\nEmpirical Methods in Natural Language Processing (EMNLP) Main, 2024\nBenchmarking Cognitive Biases in Large Language Models as Evaluators Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang\nAssociation for Computational Linguistics (ACL) Findings, 2024\nMeta-Crafting: Improved Detection of Out-of-Distributed Texts via Crafting Metadata Space Ryan Koo, Yekyung Kim, Dongyeop Kang, Jaehyung Kim\nAssociation for the Advancement of Artificial Intelligence (AAAI) Student Abstract, 2024\nCoEdIT: Text Editing by Task-Specific Instruction Tuning Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang\nEmpirical Methods in Natural Language Processing (EMNLP) Findings, 2023\n",
  "wordCount" : "158",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan Koo"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/research_interests/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ryan Koo",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Ryan Koo">
                <img src="http://localhost:1313/logork2.0.png" alt="" aria-label="logo"
                    height="18"
                    width="18">Ryan Koo</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">Ryan Koo

</div>
  </header> 
  <div class="post-content"><h2 id="research">Research</h2>
<p><small>Currently, I’m interested in developing new methods of modeling heterogeneous rewards through dense reward shaping or defining new, more grounded reward functions for language model tuning. Specifically, I’m invested in defining and developing a more informative reward space from sparse signals to better align AI systems to human behavior or preferences.</p>
<hr>
<h3 id="dynamic-multi-reward-weighting-for-multi-style-controllable-generationhttpsaclanthologyorg2024emnlp-main386"><a href="https://aclanthology.org/2024.emnlp-main.386/" target="_blank">Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation</a></h3>
<p>Karin De Langis, <strong>Ryan Koo</strong>, Dongyeop Kang<br>
<em>Empirical Methods in Natural Language Processing</em> (EMNLP) Main, 2024</p>
<h3 id="benchmarking-cognitive-biases-in-large-language-models-as-evaluatorshttpsarxivorgabs230917012"><a href="https://arxiv.org/abs/2309.17012" target="_blank">Benchmarking Cognitive Biases in Large Language Models as Evaluators</a></h3>
<p><strong>Ryan Koo</strong>, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang<br>
<em>Association for Computational Linguistics</em> (ACL) Findings, 2024</p>
<h3 id="meta-crafting-improved-detection-of-out-of-distributed-texts-via-crafting-metadata-spacehttpsojsaaaiorgindexphpaaaiarticleview30467"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/30467" target="_blank">Meta-Crafting: Improved Detection of Out-of-Distributed Texts via Crafting Metadata Space</a></h3>
<p><strong>Ryan Koo</strong>, Yekyung Kim, Dongyeop Kang, Jaehyung Kim<br>
<em>Association for the Advancement of Artificial Intelligence</em> (AAAI) Student Abstract, 2024</p>
<h3 id="coedit-text-editing-by-task-specific-instruction-tuninghttpsarxivorgabs230509857"><a href="https://arxiv.org/abs/2305.09857" target="_blank">CoEdIT: Text Editing by Task-Specific Instruction Tuning</a></h3>
<p>Vipul Raheja, Dhruv Kumar, <strong>Ryan Koo</strong>, Dongyeop Kang<br>
<em>Empirical Methods in Natural Language Processing</em> (EMNLP) Findings, 2023</p>
</small>
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Ryan Koo</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
